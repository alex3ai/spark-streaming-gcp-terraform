#!/bin/bash

# ==============================================================================
# DEPLOY COMPLETO (Infraestrutura + CÃ³digo)
# DescriÃ§Ã£o: Script all-in-one para deploy inicial
# ==============================================================================

set -e

# Cores para output
RED='\033[0;31m'
GREEN='\033[0;32m'
YELLOW='\033[1;33m'
NC='\033[0m' # No Color

log_info() {
    echo -e "${GREEN}[INFO]${NC} $1"
}

log_warn() {
    echo -e "${YELLOW}[WARN]${NC} $1"
}

log_error() {
    echo -e "${RED}[ERROR]${NC} $1"
}

# ==============================================================================
# CONFIGURAÃ‡Ã•ES
# ==============================================================================

PROJECT_ID=${GCP_PROJECT_ID:-""}
REGION=${GCP_REGION:-"us-central1"}

if [ -z "$PROJECT_ID" ]; then
    log_error "Defina GCP_PROJECT_ID"
    exit 1
fi

BUCKET_NAME="${PROJECT_ID}-data-lake"

# ==============================================================================
# BANNER
# ==============================================================================

cat << "EOF"
â•”â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•—
â•‘                                                                â•‘
â•‘       SPARK STREAMING GCP - DEPLOY AUTOMATIZADO               â•‘
â•‘                                                                â•‘
â•šâ•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•
EOF

echo ""
log_info "Projeto: ${PROJECT_ID}"
log_info "RegiÃ£o: ${REGION}"
log_info "Bucket: ${BUCKET_NAME}"
echo ""

# ==============================================================================
# ETAPA 1: VALIDAÃ‡Ã•ES PRÃ‰-DEPLOY
# ==============================================================================

log_info "ETAPA 1: Validando ambiente..."

# Verificar autenticaÃ§Ã£o
if ! gcloud auth list --filter=status:ACTIVE --format="value(account)" | grep -q .; then
    log_error "NÃ£o autenticado no GCloud"
    log_info "Execute: gcloud auth login"
    exit 1
fi

# Verificar se projeto existe
if ! gcloud projects describe ${PROJECT_ID} &>/dev/null; then
    log_error "Projeto ${PROJECT_ID} nÃ£o encontrado"
    exit 1
fi

log_info "âœ“ AutenticaÃ§Ã£o OK"

# Verificar APIs habilitadas
REQUIRED_APIS=(
    "compute.googleapis.com"
    "dataproc.googleapis.com"
    "storage.googleapis.com"
)

for api in "${REQUIRED_APIS[@]}"; do
    if gcloud services list --enabled --project=${PROJECT_ID} | grep -q ${api}; then
        log_info "âœ“ API habilitada: ${api}"
    else
        log_warn "Habilitando API: ${api}"
        gcloud services enable ${api} --project=${PROJECT_ID}
    fi
done

# ==============================================================================
# ETAPA 2: TERRAFORM INIT & PLAN
# ==============================================================================

log_info "ETAPA 2: Inicializando Terraform..."

cd terraform/environments/dev

terraform init

log_info "Executando terraform plan..."
terraform plan -var="project_id=${PROJECT_ID}" -out=tfplan

read -p "Continuar com apply? (yes/NO): " -r
echo

if [[ ! $REPLY =~ ^yes$ ]]; then
    log_error "Deploy cancelado"
    exit 0
fi

# ==============================================================================
# ETAPA 3: TERRAFORM APPLY
# ==============================================================================

log_info "ETAPA 3: Criando infraestrutura..."

terraform apply tfplan

# Extrair outputs
BUCKET_NAME=$(terraform output -raw bucket_name)
CLUSTER_NAME=$(terraform output -raw cluster_name)

cd ../../..

log_info "âœ“ Infraestrutura criada"
log_info "  Bucket: ${BUCKET_NAME}"
log_info "  Cluster: ${CLUSTER_NAME}"

# ==============================================================================
# ETAPA 4: UPLOAD DO BOOTSTRAP
# ==============================================================================

log_info "ETAPA 4: Fazendo upload do bootstrap script..."

./scripts/upload_bootstrap.sh ${BUCKET_NAME}

log_info "âœ“ Bootstrap script enviado"

# ==============================================================================
# ETAPA 5: EMPACOTAMENTO E UPLOAD DO CÃ“DIGO
# ==============================================================================

log_info "ETAPA 5: Empacotando e enviando cÃ³digo..."

./scripts/package_job.sh ${BUCKET_NAME}

log_info "âœ“ CÃ³digo enviado para GCS"

# ==============================================================================
# ETAPA 6: AGUARDAR CLUSTER
# ==============================================================================

log_info "ETAPA 6: Aguardando cluster ficar pronto..."

MAX_WAIT=600  # 10 minutos
ELAPSED=0
INTERVAL=15

while [ $ELAPSED -lt $MAX_WAIT ]; do
    STATUS=$(gcloud dataproc clusters describe ${CLUSTER_NAME} \
        --region=${REGION} \
        --project=${PROJECT_ID} \
        --format="value(status.state)" 2>/dev/null || echo "NOT_FOUND")
    
    if [ "$STATUS" == "RUNNING" ]; then
        log_info "âœ“ Cluster ativo!"
        break
    elif [ "$STATUS" == "ERROR" ]; then
        log_error "Cluster em estado de erro"
        exit 1
    fi
    
    echo -n "."
    sleep $INTERVAL
    ELAPSED=$((ELAPSED + INTERVAL))
done

if [ $ELAPSED -ge $MAX_WAIT ]; then
    log_error "Timeout aguardando cluster"
    exit 1
fi

# ==============================================================================
# RESUMO FINAL
# ==============================================================================

echo ""
echo "â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•"
log_info "âœ… DEPLOY CONCLUÃDO COM SUCESSO!"
echo "â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•"
echo ""
echo "ğŸ“Š Recursos criados:"
echo "  â€¢ Cluster Dataproc: ${CLUSTER_NAME}"
echo "  â€¢ Bucket GCS: gs://${BUCKET_NAME}"
echo "  â€¢ VPC: spark-streaming-vpc"
echo ""
echo "ğŸš€ PrÃ³ximos passos:"
echo ""
echo "1. Submeter job Spark:"
echo "   ./scripts/submit_job.sh"
echo ""
echo "2. Monitorar via Console:"
echo "   https://console.cloud.google.com/dataproc/clusters?project=${PROJECT_ID}"
echo ""
echo "3. Acessar Spark UI:"
echo "   (Via Component Gateway no console)"
echo ""
echo "âš ï¸  IMPORTANTE:"
echo "  â€¢ Cluster configurado para auto-delete apÃ³s 1h de idle"
echo "  â€¢ Execute './scripts/cleanup.sh' para destruir recursos manualmente"
echo "  â€¢ Monitore custos em: https://console.cloud.google.com/billing"
echo ""