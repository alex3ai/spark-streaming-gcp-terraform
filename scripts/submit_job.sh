#!/bin/bash
set -e

# ==============================================================================
# SRE WAR MODE: SUBMIT JOB COM CORRE√á√ÉO DE VERS√ÉO PYTHON
# ==============================================================================

# Configura√ß√µes
PROJECT_ID=${GCP_PROJECT_ID:-"spark-streaming-gcp-terraform"}
REGION=${GCP_REGION:-"us-central1"}
CLUSTER_NAME=${DATAPROC_CLUSTER:-"spark-sentiment-dev"}
BUCKET_NAME=${GCS_BUCKET:-"${PROJECT_ID}-data-lake"}

# Validar Projeto
if [ -z "$PROJECT_ID" ]; then echo "‚ùå Defina GCP_PROJECT_ID"; exit 1; fi

# 1. Empacotar (Garante que o c√≥digo novo suba)
echo "üì¶ Empacotando e enviando c√≥digo..."
./scripts/package_job.sh ${BUCKET_NAME}

# 2. Definir nome do Job
JOB_NAME="sre-validation-$(date +%Y%m%d-%H%M%S)"

echo "üöÄ Submetendo job de valida√ß√£o: ${JOB_NAME}"
echo "üìù Modo: STREAM_SOURCE=rate (Dados sint√©ticos)"

# 3. Submit com FOR√áAMENTO DE VERS√ÉO PYTHON
# Adicionamos: spark.pyspark.python e spark.pyspark.driver.python
# Isso garante que Driver e Worker usem o mesmo bin√°rio do sistema (/usr/bin/python3)

gcloud dataproc jobs submit pyspark \
    gs://${BUCKET_NAME}/jobs/sentiment.py \
    --cluster=${CLUSTER_NAME} \
    --region=${REGION} \
    --project=${PROJECT_ID} \
    --id=${JOB_NAME} \
    --py-files=gs://${BUCKET_NAME}/jobs/spark_job_package.zip \
    --properties="spark.pyspark.python=/usr/bin/python3,spark.pyspark.driver.python=/usr/bin/python3,spark.yarn.appMasterEnv.STREAM_SOURCE=rate,spark.executorEnv.STREAM_SOURCE=rate,spark.yarn.appMasterEnv.GCS_BUCKET=${BUCKET_NAME},spark.executorEnv.GCS_BUCKET=${BUCKET_NAME}" \
    --labels=env=dev,type=validation

# 4. Monitoramento
echo ""
echo "‚úÖ Job submetido com sucesso!"
echo "üëá Acompanhando logs..."
echo ""

gcloud dataproc jobs wait ${JOB_NAME} --region=${REGION} --project=${PROJECT_ID}