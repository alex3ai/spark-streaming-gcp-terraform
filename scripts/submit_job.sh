#!/bin/bash

# ==============================================================================
# SUBMIT SPARK JOB TO DATAPROC
# Descri√ß√£o: Submete job para o cluster Dataproc com corre√ß√£o de depend√™ncias e ambiente
# ==============================================================================

set -e

# ==============================================================================
# CONFIGURA√á√ïES
# ==============================================================================

# Defini√ß√µes com valores padr√£o seguros e sem espa√ßos invis√≠veis
PROJECT_ID=${GCP_PROJECT_ID:-"spark-streaming-gcp-terraform"}
REGION=${GCP_REGION:-"us-central1"}
CLUSTER_NAME=${DATAPROC_CLUSTER:-"spark-sentiment-dev"}
BUCKET_NAME=${GCS_BUCKET:-"${PROJECT_ID}-data-lake"}

# Valida√ß√µes
if [ -z "$PROJECT_ID" ]; then
    echo "‚ùå Erro: Defina GCP_PROJECT_ID"
    echo "Exemplo: export GCP_PROJECT_ID='seu-projeto-id'"
    exit 1
fi

if [ -z "$BUCKET_NAME" ]; then
    BUCKET_NAME="${PROJECT_ID}-data-lake"
    echo "‚ÑπÔ∏è Usando bucket padr√£o: ${BUCKET_NAME}"
fi

# ==============================================================================
# VERIFICA√á√ïES PR√â-SUBMIT
# ==============================================================================

echo "üîç Verificando cluster..."
if ! gcloud dataproc clusters describe ${CLUSTER_NAME} \
    --region=${REGION} \
    --project=${PROJECT_ID} &>/dev/null; then
    echo "‚ùå Cluster ${CLUSTER_NAME} n√£o encontrado!"
    echo "Execute: cd terraform/environments/dev && terraform apply"
    exit 1
fi

echo "‚úÖ Cluster ${CLUSTER_NAME} est√° ativo"

# ==============================================================================
# UPLOAD DO C√ìDIGO (Opcional)
# ==============================================================================

read -p "üì¶ Fazer upload do c√≥digo atualizado? (y/N): " -n 1 -r
echo
if [[ $REPLY =~ ^[Yy]$ ]]; then
    echo "üì§ Fazendo upload..."
    ./scripts/package_job.sh ${BUCKET_NAME}
fi

# ==============================================================================
# SUBMIT JOB
# ==============================================================================

JOB_NAME="sentiment-analysis-$(date +%Y%m%d-%H%M%S)"

echo "üöÄ Submetendo job: ${JOB_NAME}"

# --- Constru√ß√£o das Propriedades do Spark ---
# 1. Recursos (Mem√≥ria/CPU)
PROPS_RESOURCES="spark.executor.memory=2g,spark.driver.memory=2g"

# 2. Pacotes (Conector Kafka) - Injetado via properties, n√£o via flag --packages
PROPS_PACKAGES="spark.jars.packages=org.apache.spark:spark-sql-kafka-0-10_2.12:3.3.2"

# 3. Vari√°veis de Ambiente para o Spark (Resolve o erro 404 de Bucket)
# Passa o nome correto do bucket para o c√≥digo Python
PROPS_ENV="spark.yarn.appMasterEnv.GCS_BUCKET=${BUCKET_NAME},spark.executorEnv.GCS_BUCKET=${BUCKET_NAME},spark.yarn.appMasterEnv.GCP_PROJECT_ID=${PROJECT_ID},spark.executorEnv.GCP_PROJECT_ID=${PROJECT_ID}"

# --- Comando de Submiss√£o ---
gcloud dataproc jobs submit pyspark \
    gs://${BUCKET_NAME}/jobs/sentiment.py \
    --cluster=${CLUSTER_NAME} \
    --region=${REGION} \
    --project=${PROJECT_ID} \
    --id=${JOB_NAME} \
    --properties="${PROPS_RESOURCES},${PROPS_PACKAGES},${PROPS_ENV}" \
    --py-files=gs://${BUCKET_NAME}/jobs/spark_job_package.zip \
    --archives=gs://${BUCKET_NAME}/jobs/spark_job_package.zip \
    --labels=environment=dev,job-type=streaming

# ==============================================================================
# MONITORAMENTO
# ==============================================================================

echo ""
echo "‚úÖ Job submetido com sucesso!"
echo "üìä ID do Job: ${JOB_NAME}"
echo ""
echo "üîó Links √∫teis:"
echo "  Console GCP: https://console.cloud.google.com/dataproc/jobs/${JOB_NAME}?project=${PROJECT_ID}&region=${REGION}"
echo "  Logs: gcloud dataproc jobs wait ${JOB_NAME} --region=${REGION} --project=${PROJECT_ID}"
echo ""

# Perguntar se quer acompanhar logs
read -p "üìú Acompanhar logs em tempo real? (y/N): " -n 1 -r
echo
if [[ $REPLY =~ ^[Yy]$ ]]; then
    echo ""
    echo "üì° Seguindo logs (Ctrl+C para parar)..."
    gcloud dataproc jobs wait ${JOB_NAME} \
        --region=${REGION} \
        --project=${PROJECT_ID}
fi

echo ""
echo "‚úÖ Para ver outputs do driver:"
echo "   gcloud dataproc jobs describe ${JOB_NAME} --region=${REGION} --format='value(driverOutputResourceUri)' | xargs gsutil cat"