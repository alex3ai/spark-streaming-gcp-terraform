#!/bin/bash

# ==============================================================================
# SUBMIT SPARK JOB TO DATAPROC
# Descri√ß√£o: Submete job para o cluster Dataproc
# ==============================================================================

set -e

# ==============================================================================
# CONFIGURA√á√ïES
# ==============================================================================

PROJECT_ID=${GCP_PROJECT_ID:-""}
REGION=${GCP_REGION:-"us-central1"}
CLUSTER_NAME=${DATAPROC_CLUSTER:-"spark-sentiment-dev"}
BUCKET_NAME=${GCS_BUCKET:-""}

# Valida√ß√µes
if [ -z "$PROJECT_ID" ]; then
    echo "‚ùå Erro: Defina GCP_PROJECT_ID"
    echo "Exemplo: export GCP_PROJECT_ID='seu-projeto-id'"
    exit 1
fi

if [ -z "$BUCKET_NAME" ]; then
    BUCKET_NAME="${PROJECT_ID}-data-lake"
    echo "‚ÑπÔ∏è Usando bucket padr√£o: ${BUCKET_NAME}"
fi

# ==============================================================================
# VERIFICA√á√ïES PR√â-SUBMIT
# ==============================================================================

echo "üîç Verificando cluster..."
if ! gcloud dataproc clusters describe ${CLUSTER_NAME} \
    --region=${REGION} \
    --project=${PROJECT_ID} &>/dev/null; then
    echo "‚ùå Cluster ${CLUSTER_NAME} n√£o encontrado!"
    echo "Execute: terraform apply"
    exit 1
fi

echo "‚úÖ Cluster ${CLUSTER_NAME} est√° ativo"

# ==============================================================================
# UPLOAD DO C√ìDIGO (se necess√°rio)
# ==============================================================================

read -p "üì¶ Fazer upload do c√≥digo atualizado? (y/N): " -n 1 -r
echo
if [[ $REPLY =~ ^[Yy]$ ]]; then
    echo "üì§ Fazendo upload..."
    ./scripts/package_job.sh ${BUCKET_NAME}
fi

# ==============================================================================
# SUBMIT JOB
# ==============================================================================

JOB_NAME="sentiment-analysis-$(date +%Y%m%d-%H%M%S)"

echo "üöÄ Submetendo job: ${JOB_NAME}"

gcloud dataproc jobs submit pyspark \
    gs://${BUCKET_NAME}/jobs/sentiment.py \
    --cluster=${CLUSTER_NAME} \
    --region=${REGION} \
    --project=${PROJECT_ID} \
    --id=${JOB_NAME} \
    --properties=spark.executor.memory=2g,spark.driver.memory=2g
    --py-files=gs://${BUCKET_NAME}/jobs/spark_job_package.zip \
    --labels=environment=dev,job-type=streaming

# ==============================================================================
# MONITORAMENTO
# ==============================================================================

echo ""
echo "‚úÖ Job submetido com sucesso!"
echo "üìä ID do Job: ${JOB_NAME}"
echo ""
echo "üîó Links √∫teis:"
echo "  Console GCP: https://console.cloud.google.com/dataproc/jobs/${JOB_NAME}?project=${PROJECT_ID}"
echo "  Logs: gcloud dataproc jobs wait ${JOB_NAME} --region=${REGION} --project=${PROJECT_ID}"
echo ""

# Perguntar se quer acompanhar logs
read -p "üìú Acompanhar logs em tempo real? (y/N): " -n 1 -r
echo
if [[ $REPLY =~ ^[Yy]$ ]]; then
    gcloud dataproc jobs wait ${JOB_NAME} \
        --region=${REGION} \
        --project=${PROJECT_ID}
fi