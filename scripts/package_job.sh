#!/bin/bash

# ==============================================================================
# PACKAGE JOB FOR GCP SUBMISSION
# DescriÃ§Ã£o: Empacota o cÃ³digo Python e faz upload para GCS
# ==============================================================================

set -e

# Verificar argumentos
if [ $# -ne 1 ]; then
    echo "Uso: $0 BUCKET_NAME"
    exit 1
fi

BUCKET_NAME=$1
OUTPUT_FILE="spark_job_package.zip"

echo "ðŸ“¦ Empacotando job Spark..."

# Remover pacote antigo para garantir que nÃ£o haja lixo
rm -f ${OUTPUT_FILE}

# ------------------------------------------------------------------------------
# CRIAÃ‡ÃƒO DO ZIP
# O segredo aqui Ã© zipar a pasta 'app/' recursivamente.
# Isso cria um zip que contÃ©m a pasta 'app' na raiz.
# Quando o Spark descompacta, ele vÃª a pasta 'app', permitindo:
# "from app.config import settings"
# ------------------------------------------------------------------------------
zip -r ${OUTPUT_FILE} app/ \
    -x "*.pyc" \
    -x "*__pycache__/*" \
    -x "app/docs/*" \
    -x "app/scripts/*" \
    -x "*.DS_Store"

echo "âœ… Pacote criado: ${OUTPUT_FILE}"

# Upload para GCS
echo "ðŸ“¤ Fazendo upload do pacote para GCS..."
gsutil cp ${OUTPUT_FILE} gs://${BUCKET_NAME}/jobs/

# Upload do arquivo principal do job (sentiment.py) separadamente
# O arquivo principal fica fora do zip para ser o ponto de entrada
echo "ðŸ“¤ Fazendo upload do job principal..."
gsutil cp app/jobs/sentiment.py gs://${BUCKET_NAME}/jobs/

echo "âœ… Upload concluÃ­do!"
echo "ðŸ“¦ Pacote: gs://${BUCKET_NAME}/jobs/${OUTPUT_FILE}"
echo "ðŸ“„ Main Job: gs://${BUCKET_NAME}/jobs/sentiment.py"